{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brief-adolescent",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "importing Jupyter notebook from DataInfo.ipynb\n"
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import timeit\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "%matplotlib inline\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqIO.FastaIO import FastaIterator\n",
    "from collections import Counter\n",
    "from itertools import islice, groupby\n",
    "from collections import Counter, namedtuple\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import clear_output # clear_output(wait=True)\n",
    "\n",
    "import import_ipynb\n",
    "from DataInfo import currDataset, datasetPath, matchPattern, cutter, cutterLen, fragmentN, commonCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elegant-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFasta(currDataset, datasetPath, matchPattern, matchMode = False):\n",
    "    print(f'...start parsing {currDataset} fasta file ...')\n",
    "    start = time.time()\n",
    "    fasta_sequences = SeqIO.parse(open(datasetPath), \"fasta\")\n",
    "    seqs = matchMode and [ i.seq for i in fasta_sequences if re.search(matchPattern, i.id) ] or [ i.seq for i in fasta_sequences ]\n",
    "    end = time.time()\n",
    "    print(f'...cost{ end-start } sec to parse fasta file ...')\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSeq(seq, cutter):\n",
    "    targetNum = seq.count(cutter)\n",
    "    parseResult = seq.split(cutter)\n",
    "    fragmentLength = [len(read) for read in parseResult]   \n",
    "    return targetNum, parseResult, fragmentLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSeqByCutter(parseFastaSeqs): \n",
    "    start = time.time()\n",
    "    print(f'...start parse seq by cutter: { cutter }')\n",
    "    fragmentsLenList = []\n",
    "    fragmentsSeqList = []\n",
    "    for seq in parseFastaSeqs:\n",
    "        targetNum, fragmentSeq, fragmentLength = parseSeq(seq, cutter)\n",
    "        fragmentsLenList.append(fragmentLength)\n",
    "        fragmentsSeqList.append(fragmentSeq)\n",
    "    end = time.time()\n",
    "    print(f'...cost { end-start } sec to cut sequence')\n",
    "    return fragmentsLenList, fragmentsSeqList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSeqId(fileName):\n",
    "    fasta_sequences = SeqIO.parse(open(fileName), \"fasta\")\n",
    "    seqIds = [i.id for i in fasta_sequences]\n",
    "    return seqIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "typical-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportCsvFile(fileName, rowList):\n",
    "    with open(fileName, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['length','uniqueCount'])\n",
    "        for row in rowList:\n",
    "            writer.writerows(rowList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "valued-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqInfo(dataName, seqs):\n",
    "    seqNum = len(seqs)\n",
    "    sumLen = sum([len(i) for i in seqs])\n",
    "    print(f'{dataName} dataset\\n number of sequence:{seqNum}\\n total length:{sumLen}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "express-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nGramLength(lengthList, count, N):\n",
    "    nGramLegList = []\n",
    "    for i in range(count-N+1):\n",
    "        nGramLegList.append(sum(lengthList[i:i+N]))\n",
    "    return nGramLegList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "local-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSeqGenerator(length):\n",
    "    seq = ''.join(random.choice('ACTG') for _ in range(length))\n",
    "    return Seq(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifragmentList(fragmentsLenList, fragmentsSeqList):\n",
    "    flattenLenList = [item for subList in fragmentsLenList for item in subList]\n",
    "    lenCounter = Counter(flattenLenList)\n",
    "    flattenSeqList = [item for subList in fragmentsSeqList for item in subList]\n",
    "    flattenSeqList.sort(key=len)\n",
    "    uniqueLen = namedtuple('fragments',['length','uniqueCount'])\n",
    "    uniqueLenList = []\n",
    "    classifiedLenSeqs = ({k: list(v) for k, v in groupby(sorted(flattenSeqList, key=len), key=len)})\n",
    "    for key, value in classifiedLenSeqs.items():\n",
    "        tem = uniqueLen(key, len(set(value)))\n",
    "        uniqueLenList.append(tem)\n",
    "    sortedUniLenList = sorted(uniqueLenList, key=lambda x: x.uniqueCount, reverse=True)\n",
    "    print(f\"The most common length of fragments:\\n {sortedUniLenList[:10]}\")\n",
    "    exportCsvFile(\"table.csv\", sortedUniLenList[:100])\n",
    "    return uniqueLenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fragment length distribution plot\n",
    "def fragmentLenPlot(uniqueLenList):\n",
    "    xInterval = 1\n",
    "    x_axiMax = uniqueLenList[-1].length\n",
    "    x = np.arange(0, x_axiMax, xInterval)\n",
    "    y = [sum(map(lambda x: x.uniqueCount, uniqueLenList[j:j+xInterval])) for j in range(0, x_axiMax, xInterval) ]\n",
    "    plt.figure(figsize=(20,12)) \n",
    "    plt.title(\"Length of unique fragments' Distribution\")\n",
    "    plt.xlabel(\"length\", fontsize=20)\n",
    "    plt.ylabel(\"occurrences\", fontsize=20)\n",
    "    plt.bar(x[:20000], y[:20000], color='orange')\n",
    "    plt.xlim([0, 2000])\n",
    "    plt.show()\n",
    "    return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}